# 开篇
## 开篇：帮助初次学习缓存项目的人能够更快的上手项目的开发

#### 项目名称：KamaCache

#### 项目地址：[GitHub - youngyangyang04/KamaCache: 【代码随想录知识星球】项目分享-cache的实现（LRU，LFU，ARC）](https://github.com/youngyangyang04/KamaCache)

**此项目使用多个页面替换策略实现了一个线程安全的缓存系统：**

- LRU：最近最久未使用页面置换算法
- LFU：最不经常使用页面置换算法
- ARC：自适应替换缓存算法

## 引言

缓存策略在系统和实际业务开发过程中较为常见，下面我为大家列出一些使用了缓存策略的知名系统和组件：

### 操作系统与存储系统

#### Linux Kernel：

- **LRU:**

- 用于内存管理中页面缓存(Page Cache)，当物理内存不足时，系统会优先淘汰最近未使用的页面。
- 内核的虚拟文件系统(VFS)也采用LRU维护缓存的元数据

#### Redis：

- **LRU和LFU：**

- Redis 本身并不直接实现 LRU 缓存策略，但它提供了过期和淘汰机制。在内存不足时，Redis 可以配置不同的淘汰策略，如 `volatile-lru`（对于设置了过期时间的键，使用 LRU 策略淘汰数据）。
- Redis 提供了 LFU 策略（从 4.0 版本开始）作为其内存驱逐策略之一，可以通过 `maxmemory-policy` 配置。

#### ZFS（Zettabyte File System）

- **ARC:**

- ZFS文件系统中的缓存管理主要基于ARC
- ARC优于单纯的LRU或LFU，因为它能动态调整缓存策略以适应不同的访问模式

#### PostgreSQL

- **ARC:**

- 场景：在一些PostgreSQL的缓存拓展中实现了基于ARC的缓存机制

#### Hadoop

- Hadoop(改进实现)

- 场景：某些分布式存储的改进版本中，使用了类似ARC的动态缓存替换策略

### 网络与负载均衡

#### NGINX

- **LRU:**

- 用于缓存代理模式下的响应数据，淘汰最久未使用的缓存条目

### 嵌入式系统与浏览器

#### Android内存缓存

- **LRU:**

- Android的LruCache类用于管理有限内存中的应用数据缓存。

#### Web浏览器

- **LRU:**

- 浏览器的缓存机制（如历史记录、图片缓存）使用LRU策略来淘汰过期数据。

# 项目背景介绍
### 什么是缓存？

缓存是将高频访问的数据暂存到内存中，是加速数据访问的存储，降低延迟，提高吞吐率的利器。

### 为什么要实现缓存系统?

因缓存的使用相关需求，通过牺牲一部分服务器内存，减少对磁盘或者数据库资源进行直接读写，可换取更快响应速度，尤其是处理高并发的场景，负责存储经常访问的数据，通过设计合理的缓存机制提高资源的访问效率。由于服务器的内存是有限的，我们不能把所有数据都存放在内存中，因此需要一种机制来决定当使用内存超过一定标准时，应该删除哪些数据，这就涉及到缓存淘汰策略的选择。

### 在什么地方加缓存？

ps: 这里我们引用这篇文章中的解释：[https://blog.csdn.net/chongfa2008/article/details/121956961](https://blog.csdn.net/chongfa2008/article/details/121956961)

缓存对于每个开发者来说是相当熟悉了，为了提高程序的性能我们会去加缓存，但是在什么地方加缓存，如何加缓存呢？

举个例子：假设一个网站，需要提高性能，缓存可以放在浏览器，可以放在反向代理服务器，还可以放在应用程序进程内，同时可以放在分布式缓存系统中。
![[缓存系统1.png]]


从用户请求数据到数据返回，数据经过了浏览器、CDN、代理服务器、应用服务器，以及数据库各个环节。每个环节都可以运用缓存技术。从浏览器/客户端开始请求数据，通过HTTP配合CDN获取数据的变更情况，到达代理服务器(Nginx)可以通过反向代理获取静态资源。再往下来到应用服务器可以通过进程内（堆内）缓存，分布式缓存等方式获取数据。如果以上所有缓存都没有命中数据，才会回源到数据库。

缓存的顺序：用户请求->HTTP缓存->CDN缓存->代理服务器缓存->进程内缓存->分布式缓存->数据库

距离用户越近，缓存能够发挥的效果越好。

# 缓存淘汰策略
## 缓存淘汰策略

常见的有FIFO、LRU、LFU、ARC等，下面我将为大家详细介绍以上四种缓存淘汰策略。ps：前三种缓存淘汰策略的基础版本我会简略带过，因为网上的资料很多且详细，**我重点讲一下LRU和LFU的待优化点**，最后一种缓存淘汰策略ARC我会详细介绍一下。

缓存污染：指访问很少的数据在服务完访问请求后还继续留在缓存中，造成缓存空间的浪费。

缓存污染一旦变严重后，有大量不再访问的数据滞留在缓存中，往缓存中写入新数据时需要先把数据淘汰出缓存，引入额外的时间开销。

### FIFO

先进先出算法，是最简单的页面替换策略，按照页面进入内存的顺序来决定替换哪个页面。即优先淘汰最早进入内存的页面，不论这些页面之后是否被频繁访问。但是FIFO可能会导致“Belady异常”，即随着分配给进程的物理块数增加，缺页次数反而增加。

### LRU

最近最少使用算法，如果一个数据最近被访问过，那么将来被访问的可能性也较大。因此，它选择最近最长时间未被访问的页面进行替换。LRU的性能和效率接近OPT，但是对于频繁访问的页面更新开销较大。

潜在问题：淘汰热点数据，如果有个数据在1个小时的前59分钟访问了1万次(可见这是个热点数据),再后一分钟没有访问这个数据，但是有其他的数据访问，就导致了我们这个热点数据被淘汰。

推荐一篇学习LRU算法的文章：[https://blog.csdn.net/saxon_li/article/details/123974696](https://blog.csdn.net/saxon_li/article/details/123974696)

**基础LRU代码**

```
// 前向声明
template<typename Key, typename Value> class KLruCache;

template<typename Key, typename Value>
class LruNode 
{
private:
    Key key_;
    Value value_;
    size_t accessCount_;  // 访问次数
    std::weak_ptr<LruNode<Key, Value>> prev_;  // 改为weak_ptr打破循环引用
    std::shared_ptr<LruNode<Key, Value>> next_;

public:
    LruNode(Key key, Value value)
        : key_(key)
        , value_(value)
        , accessCount_(1) 
    {}

    // 提供必要的访问器
    Key getKey() const { return key_; }
    Value getValue() const { return value_; }
    void setValue(const Value& value) { value_ = value; }
    size_t getAccessCount() const { return accessCount_; }
    void incrementAccessCount() { ++accessCount_; }

    friend class KLruCache<Key, Value>;
};


template<typename Key, typename Value>
class KLruCache : public KICachePolicy<Key, Value>
{
public:
    using LruNodeType = LruNode<Key, Value>;
    using NodePtr = std::shared_ptr<LruNodeType>;
    using NodeMap = std::unordered_map<Key, NodePtr>;

    KLruCache(int capacity)
        : capacity_(capacity)
    {
        initializeList();
    }

    ~KLruCache() override = default;

    // 添加缓存
    void put(Key key, Value value) override
    {
        if (capacity_ <= 0)
            return;
    
        std::lock_guard<std::mutex> lock(mutex_);
        auto it = nodeMap_.find(key);
        if (it != nodeMap_.end())
        {
            // 如果在当前容器中,则更新value,并调用get方法，代表该数据刚被访问
            updateExistingNode(it->second, value);
            return ;
        }

        addNewNode(key, value);
    }

    bool get(Key key, Value& value) override
    {
        std::lock_guard<std::mutex> lock(mutex_);
        auto it = nodeMap_.find(key);
        if (it != nodeMap_.end())
        {
            moveToMostRecent(it->second);
            value = it->second->getValue();
            return true;
        }
        return false;
    }

    Value get(Key key) override
    {
        Value value{};
        // memset(&value, 0, sizeof(value));   // memset 是按字节设置内存的，对于复杂类型（如 string）使用 memset 可能会破坏对象的内部结构
        get(key, value);
        return value;
    }

    // 删除指定元素
    void remove(Key key) 
    {   
        std::lock_guard<std::mutex> lock(mutex_);
        auto it = nodeMap_.find(key);
        if (it != nodeMap_.end())
        {
            removeNode(it->second);
            nodeMap_.erase(it);
        }
    }

private:
    void initializeList()
    {
        // 创建首尾虚拟节点
        dummyHead_ = std::make_shared<LruNodeType>(Key(), Value());
        dummyTail_ = std::make_shared<LruNodeType>(Key(), Value());
        dummyHead_->next_ = dummyTail_;
        dummyTail_->prev_ = dummyHead_;
    }

    void updateExistingNode(NodePtr node, const Value& value) 
    {
        node->setValue(value);
        moveToMostRecent(node);
    }

    void addNewNode(const Key& key, const Value& value) 
    {
       if (nodeMap_.size() >= capacity_) 
       {
           evictLeastRecent();
       }

       NodePtr newNode = std::make_shared<LruNodeType>(key, value);
       insertNode(newNode);
       nodeMap_[key] = newNode;
    }

    // 将该节点移动到最新的位置
    void moveToMostRecent(NodePtr node) 
    {
        removeNode(node);
        insertNode(node);
    }

    void removeNode(NodePtr node) 
    {
        if(!node->prev_.expired() && node->next_) 
        {
            auto prev = node->prev_.lock(); // 使用lock()获取shared_ptr
            prev->next_ = node->next_;
            node->next_->prev_ = prev;
            node->next_ = nullptr; // 清空next_指针，彻底断开节点与链表的连接
        }
    }

    // 从尾部插入结点
    void insertNode(NodePtr node) 
    {
        node->next_ = dummyTail_;
        node->prev_ = dummyTail_->prev_;
        dummyTail_->prev_.lock()->next_ = node; // 使用lock()获取shared_ptr
        dummyTail_->prev_ = node;
    }

    // 驱逐最近最少访问
    void evictLeastRecent() 
    {
        NodePtr leastRecent = dummyHead_->next_;
        removeNode(leastRecent);
        nodeMap_.erase(leastRecent->getKey());
    }

private:
    int           capacity_; // 缓存容量
    NodeMap       nodeMap_; // key -> Node 
    std::mutex    mutex_;
    NodePtr       dummyHead_; // 虚拟头结点
    NodePtr       dummyTail_;
};
```

学习完基础的LRU缓存算法实现之后，大家不难发现他身上的缺点：

- 对访问模式不敏感：如果是循环的一次性遍历大量不重复的数据（如A->B->C->D->A->B->...），LRU可能逐步清空，几乎无法命中。
- 缓存污染：如果加载一些不再会被访问的冷数据（如一次性数据），将原有的热点数据挤出，冷数据留在缓存中，降低了缓存的利用率。
- 不适用于某些场景：在某些场景下，最近最少使用并不代表最不重要或最少需要。
- 锁的粒度大：多线程高并发的访问下，同步等待将是一笔极大的时间开销。

针对上述问题，可以本文进行了以下几点优化：

- LRU-k
- HashLRU

#### LRU-k

参考：[https://juejin.cn/post/6844904049263771662](https://juejin.cn/post/6844904049263771662)

LRU-k算法是对LRU算法的改进，基础的LRU算法被访问数据进入缓存队列只需要访问(`put、get`)一次就行，但是现在需要被访问k（大小自定义）次才能被放入缓存中，基础的LRU算法可以看成是LRU-1。

LRU-k算法有两个队列一个是缓存队列，一个是数据访问历史队列。当访问一个数据时，首先将其添加进入访问历史队列并进行累加访问次数，当该数据的访问次数超过k次后，才将数据缓存到缓存队列，从而避免缓存队列被冷数据所污染。同时访问历史队列中的数据也不是一直保留的，也是需要按照LRU的规则进行淘汰的。LRU-k执行过程如图：

![[缓存系统2.png]]


一般情况下，当k的值越大，缓存的命中率越高，但也使得缓存难以淘汰。综合来说，k = 2 时性能最优。

**LRU-k 代码实现**

```
// LRU优化：Lru-k版本。 通过继承的方式进行再优化
template<typename Key, typename Value>
class KLruKCache : public KLruCache<Key, Value>
{
public:
    KLruKCache(int capacity, int historyCapacity, int k)
        : KLruCache<Key, Value>(capacity) // 调用基类构造
        , historyList_(std::make_unique<KLruCache<Key, size_t>>(historyCapacity))
        , k_(k)
    {}

    Value get(Key key) 
    {
        // 首先尝试从主缓存获取数据
        Value value{};
        bool inMainCache = KLruCache<Key, Value>::get(key, value);

        // 获取并更新访问历史计数
        size_t historyCount = historyList_->get(key);
        historyCount++;
        historyList_->put(key, historyCount);

        // 如果数据在主缓存中，直接返回
        if (inMainCache) 
        {
            return value;
        }

        // 如果数据不在主缓存，但访问次数达到了k次
        if (historyCount >= k_) 
        {
            // 检查是否有历史值记录
            auto it = historyValueMap_.find(key);
            if (it != historyValueMap_.end()) 
            {
                // 有历史值，将其添加到主缓存
                Value storedValue = it->second;
                
                // 从历史记录移除
                historyList_->remove(key);
                historyValueMap_.erase(it);
                
                // 添加到主缓存
                KLruCache<Key, Value>::put(key, storedValue);
                
                return storedValue;
            }
            // 没有历史值记录，无法添加到缓存，返回默认值
        }

        // 数据不在主缓存且不满足添加条件，返回默认值
        return value;
    }

    void put(Key key, Value value) 
    {
        // 检查是否已在主缓存
        Value existingValue{};
        bool inMainCache = KLruCache<Key, Value>::get(key, existingValue);
        
        if (inMainCache) 
        {
            // 已在主缓存，直接更新
            KLruCache<Key, Value>::put(key, value);
            return;
        }
        
        // 获取并更新访问历史
        size_t historyCount = historyList_->get(key);
        historyCount++;
        historyList_->put(key, historyCount);
        
        // 保存值到历史记录映射，供后续get操作使用
        historyValueMap_[key] = value;
        
        // 检查是否达到k次访问阈值
        if (historyCount >= k_) 
        {
            // 达到阈值，添加到主缓存
            historyList_->remove(key);
            historyValueMap_.erase(key);
            KLruCache<Key, Value>::put(key, value);
        }
    }

private:
    int                                     k_; // 进入缓存队列的评判标准
    std::unique_ptr<KLruCache<Key, size_t>> historyList_; // 访问数据历史记录(value为访问次数)
    std::unordered_map<Key, Value>          historyValueMap_; // 存储未达到k次访问的数据值
};
```

上述代码直接继承基础LRU。

#### HashLRU

参考：[https://github.com/songangweb/mcache/tree/master](https://github.com/songangweb/mcache/tree/master)

有一定开发经验的同学一定发现了，该项目中锁的粒度很大，并且这个锁的粒度还不好直接减少毕竟涉及线程安全的地方实在是太多了，甚至连使用读写锁都没办法优化。那我们只好换个思路，将lru分片。 具体实现结构如图：

![[代码随想录HashLRU.png]]

根据传入的`key`值进行哈希运算后找到对应的`lru`分片，然后调用该分片相应的方法。

大家试想一下当多个线程同时访问一个LRU时（LFU同理），由于锁的粒度大，是不是会造成长时间的同步等待，那如果是多个线程同时访问多个LRU缓存呢，是不是同步等待时间大大减少了。

**普通的**`**lruCache**`**和**`**lfuCache**`**在高并发情况下耗时增加的原因分析：**

线程安全的lfuCache中有锁的存在。每次读写操作之前都有加锁操作，完成读写操作之后还有解锁操作。在低QPS下，锁的竞争的耗时基本可以忽略；但在高并发的情况下，大量的时间消耗在等待锁的操作上，导致耗时增长。

`**HashLruCache**`**和**`**HashLfuCache**`**适应高并发场景：**

针对大量同步等待操作导致耗时增加的情况，解决方案就是尽量减小临界区。引入`hash`机制，对全量数据做分片处理，在原有`LfuCache`的基础上形成`HashLfuCache`，以降低查询耗时。

`HashLfuCache`引入哈希算法，将缓存数据分散到N个`LfuCache`上，查询时也按照相同的哈希算法，先获取数据可能存在的分片，然后再去对应的分片上查询数据。这样可以增加lfu的读写操作的并行度，减少同步等待的耗时。

`HashLru`代码实现：

```
// lru优化：对lru进行分片，提高高并发使用的性能
template<typename Key, typename Value>
class KHashLruCaches
{
public:
    KHashLruCaches(size_t capacity, int sliceNum)
        : capacity_(capacity)
        , sliceNum_(sliceNum > 0 ? sliceNum : std::thread::hardware_concurrency())
    {
        size_t sliceSize = std::ceil(capacity / static_cast<double>(sliceNum_)); // 获取每个分片的大小
        for (int i = 0; i < sliceNum_; ++i)
        {
            lruSliceCaches_.emplace_back(new KLruCache<Key, Value>(sliceSize)); 
        }
    }

    void put(Key key, Value value)
    {
        // 获取key的hash值，并计算出对应的分片索引
        size_t sliceIndex = Hash(key) % sliceNum_;
        lruSliceCaches_[sliceIndex]->put(key, value);
    }

    bool get(Key key, Value& value)
    {
        // 获取key的hash值，并计算出对应的分片索引
        size_t sliceIndex = Hash(key) % sliceNum_;
        return lruSliceCaches_[sliceIndex]->get(key, value);
    }

    Value get(Key key)
    {
        Value value;
        memset(&value, 0, sizeof(value));
        get(key, value);
        return value;
    }

private:
    // 将key转换为对应hash值
    size_t Hash(Key key)
    {
        std::hash<Key> hashFunc;
        return hashFunc(key);
    }

private:
    size_t                                              capacity_;  // 总容量
    int                                                 sliceNum_;  // 切片数量
    std::vector<std::unique_ptr<KLruCache<Key, Value>>> lruSliceCaches_; // 切片LRU缓存
};
```

`HashLruCaches`和`KLruCache`类是组合关系，前者依赖于后者，`KLruCache`类的实现就在上文中。

注意：`LRU(LFU)`分片之后淘汰的元素就不是全局中访问最近最不经常访问的元素了(全局的`minFreq_`)，淘汰的就是局部`minFreq_`，这里是可以接受的，因为可能某个分片LRU(LFU)中存着大量元素，下一个元素还分给这个`LRU(LFU)`分片的话，就应该将当前的`LRU(LFU)`中最近最不经常访问的元素剔除而不是全局的最近最不经常访问元素，然后将新元素添加进来。

### LFU

最近使用频率高的数据很大概率将会再次被使用,而最近使用频率低的数据,将来大概率不会再使用。

做法：把使用频率最小的数据置换出去。这种算法更多是从使用频率的角度（但是当缓存满时，如果最低访问频次的缓存数据有多个，就需要考虑哪个元素最近最久未被访问）去考虑的。

附上一篇学习LFU的好文：[https://blog.csdn.net/saxon_li/article/details/123985667](https://blog.csdn.net/saxon_li/article/details/123985667)

学完基础的LFU，大家不难发现其身上还有很多待优化点：

- 频率爆炸问题：对于长期驻留在缓存中的热数据，频率计数可能会无限增长，占用额外的存储空间或导致计数溢出。
- 过时热点数据占用缓存：一些数据可能已经不再是热点数据，但因访问频率过高，难以被替换。
- 冷启动问题：刚加入缓存的项可能因为频率为1而很快被淘汰，即便这些项是近期访问的热门数据。
- 不适合短期热点：LFU对长期热点数据表现较好，但对短期热点数据响应较慢，可能导致短期热点数据无法及时缓存。
- 缺乏动态适应性：固定的LFU策略难以适应不同的应用场景或工作负载。
- 锁的粒度大，多线程高并发访问下锁的同步等待时间过长。

针对上述问题，文本进行了以下几点优化：

- 加上最大平均访问次数限制
- HashLFUCache

#### 最大平均访问次数限制

参考：[https://blog.csdn.net/yunshuipiao123/article/details/98958750](https://blog.csdn.net/yunshuipiao123/article/details/98958750)

在LFU算法之上，引入访问次数平均值概念，当平均值大于最大平均值限制时将所有结点的访问次数减去最大平均值限制的一半或者一个固定值。相当于热点数据“老化”了，这样可以避免频次计数溢出，也可以缓解缓存污染。

**加上最大平均访问次数限制的LFU代码**

```
template<typename Key, typename Value> class KLfuCache;

template<typename Key, typename Value>
class FreqList
{
private:
    struct Node
    {
        int freq; // 访问频次
        Key key;
        Value value;
        std::weak_ptr<Node> pre; // 上一结点改为weak_ptr打破循环引用
        std::shared_ptr<Node> next;

        Node() 
        : freq(1), next(nullptr) {}
        Node(Key key, Value value) 
        : freq(1), key(key), value(value), next(nullptr) {}
    };

    using NodePtr = std::shared_ptr<Node>;
    int freq_; // 访问频率
    NodePtr head_; // 假头结点
    NodePtr tail_; // 假尾结点

public:
    explicit FreqList(int n) 
     : freq_(n) 
    {
      head_ = std::make_shared<Node>();
      tail_ = std::make_shared<Node>();
      head_->next = tail_;
      tail_->pre = head_;
    }

    bool isEmpty() const
    {
      return head_->next == tail_;
    }

    // 提那家结点管理方法
    void addNode(NodePtr node) 
    {
        if (!node || !head_ || !tail_) 
            return;

        node->pre = tail_->pre;
        node->next = tail_;
        tail_->pre.lock()->next = node; // 使用lock()获取shared_ptr
        tail_->pre = node;
    }

    void removeNode(NodePtr node)
    {
        if (!node || !head_ || !tail_)
            return;
        if (node->pre.expired() || !node->next) 
            return;

        auto pre = node->pre.lock(); // 使用lock()获取shared_ptr
        pre->next = node->next;
        node->next->pre = pre;
        node->next = nullptr; // 确保显式置空next指针，彻底断开节点与链表的连接
    }

    NodePtr getFirstNode() const { return head_->next; }
    
    friend class KLfuCache<Key, Value>;
};

template <typename Key, typename Value>
class KLfuCache : public KICachePolicy<Key, Value>
{
public:
    using Node = typename FreqList<Key, Value>::Node;
    using NodePtr = std::shared_ptr<Node>;
    using NodeMap = std::unordered_map<Key, NodePtr>;

    KLfuCache(int capacity, int maxAverageNum = 10)
    : capacity_(capacity), minFreq_(INT8_MAX), maxAverageNum_(maxAverageNum),
      curAverageNum_(0), curTotalNum_(0) 
    {}

    ~KLfuCache() override = default;

    void put(Key key, Value value) override
    {
        if (capacity_ == 0)
            return;

        std::lock_guard<std::mutex> lock(mutex_);
        auto it = nodeMap_.find(key);
        if (it != nodeMap_.end())
        {
            // 重置其value值
            it->second->value = value;
            // 找到了直接调整就好了，不用再去get中再找一遍，但其实影响不大
            getInternal(it->second, value);
            return;
        }

        putInternal(key, value);
    }

    // value值为传出参数
    bool get(Key key, Value& value) override
    {
      std::lock_guard<std::mutex> lock(mutex_);
      auto it = nodeMap_.find(key);
      if (it != nodeMap_.end())
      {
          getInternal(it->second, value);
          return true;
      }

      return false;
    }

    Value get(Key key) override
    {
      Value value;
      get(key, value);
      return value;
    }

      // 清空缓存,回收资源
    void purge()
    {
      nodeMap_.clear();
      freqToFreqList_.clear();
    }

private:
    void putInternal(Key key, Value value); // 添加缓存
    void getInternal(NodePtr node, Value& value); // 获取缓存

    void kickOut(); // 移除缓存中的过期数据

    void removeFromFreqList(NodePtr node); // 从频率列表中移除节点
    void addToFreqList(NodePtr node); // 添加到频率列表

    void addFreqNum(); // 增加平均访问等频率
    void decreaseFreqNum(int num); // 减少平均访问等频率
    void handleOverMaxAverageNum(); // 处理当前平均访问频率超过上限的情况
    void updateMinFreq();

private:
    int                                            capacity_; // 缓存容量
    int                                            minFreq_; // 最小访问频次(用于找到最小访问频次结点)
    int                                            maxAverageNum_; // 最大平均访问频次
    int                                            curAverageNum_; // 当前平均访问频次
    int                                            curTotalNum_; // 当前访问所有缓存次数总数 
    std::mutex                                     mutex_; // 互斥锁
    NodeMap                                        nodeMap_; // key 到 缓存节点的映射
    std::unordered_map<int, FreqList<Key, Value>*> freqToFreqList_;// 访问频次到该频次链表的映射
};

template<typename Key, typename Value>
void KLfuCache<Key, Value>::getInternal(NodePtr node, Value& value)
{
    // 找到之后需要将其从低访问频次的链表中删除，并且添加到+1的访问频次链表中，
    // 访问频次+1, 然后把value值返回
    value = node->value;
    // 从原有访问频次的链表中删除节点
    removeFromFreqList(node); 
    node->freq++;
    addToFreqList(node);
    // 如果当前node的访问频次如果等于minFreq+1，并且其前驱链表为空，则说明
    // freqToFreqList_[node->freq - 1]链表因node的迁移已经空了，需要更新最小访问频次
    if (node->freq - 1 == minFreq_ && freqToFreqList_[node->freq - 1]->isEmpty())
        minFreq_++;

    // 总访问频次和当前平均访问频次都随之增加
    addFreqNum();
}

template<typename Key, typename Value>
void KLfuCache<Key, Value>::putInternal(Key key, Value value)
{   
    // 如果不在缓存中，则需要判断缓存是否已满
    if (nodeMap_.size() == capacity_)
    {
        // 缓存已满，删除最不常访问的结点，更新当前平均访问频次和总访问频次
        kickOut();
    }
    
    // 创建新结点，将新结点添加进入，更新最小访问频次
    NodePtr node = std::make_shared<Node>(key, value);
    nodeMap_[key] = node;
    addToFreqList(node);
    addFreqNum();
    minFreq_ = std::min(minFreq_, 1);
}

template<typename Key, typename Value>
void KLfuCache<Key, Value>::kickOut()
{
    NodePtr node = freqToFreqList_[minFreq_]->getFirstNode();
    removeFromFreqList(node);
    nodeMap_.erase(node->key);
    decreaseFreqNum(node->freq);
}

template<typename Key, typename Value>
void KLfuCache<Key, Value>::removeFromFreqList(NodePtr node)
{
    // 检查结点是否为空
    if (!node) 
        return;
    
    auto freq = node->freq;
    freqToFreqList_[freq]->removeNode(node);
}

template<typename Key, typename Value>
void KLfuCache<Key, Value>::addToFreqList(NodePtr node)
{
    // 检查结点是否为空
    if (!node) 
        return;

    // 添加进入相应的频次链表前需要判断该频次链表是否存在
    auto freq = node->freq;
    if (freqToFreqList_.find(node->freq) == freqToFreqList_.end())
    {
        // 不存在则创建
        freqToFreqList_[node->freq] = new FreqList<Key, Value>(node->freq);
    }

    freqToFreqList_[freq]->addNode(node);
}

template<typename Key, typename Value>
void KLfuCache<Key, Value>::addFreqNum()
{
    curTotalNum_++;
    if (nodeMap_.empty())
        curAverageNum_ = 0;
    else
        curAverageNum_ = curTotalNum_ / nodeMap_.size();

    if (curAverageNum_ > maxAverageNum_)
    {
       handleOverMaxAverageNum();
    }
}

template<typename Key, typename Value>
void KLfuCache<Key, Value>::decreaseFreqNum(int num)
{
    // 减少平均访问频次和总访问频次
    curTotalNum_ -= num;
    if (nodeMap_.empty())
        curAverageNum_ = 0;
    else
        curAverageNum_ = curTotalNum_ / nodeMap_.size();
}

template<typename Key, typename Value>
void KLfuCache<Key, Value>::handleOverMaxAverageNum()
{
    if (nodeMap_.empty())
        return;

    // 当前平均访问频次已经超过了最大平均访问频次，所有结点的访问频次- (maxAverageNum_ / 2)
    for (auto it = nodeMap_.begin(); it != nodeMap_.end(); ++it)
    {
        // 检查结点是否为空
        if (!it->second)
            continue;

        NodePtr node = it->second;

        // 先从当前频率列表中移除
        removeFromFreqList(node);

        // 减少频率
        node->freq -= maxAverageNum_ / 2;
        if (node->freq < 1) node->freq = 1;

        // 添加到新的频率列表
        addToFreqList(node);
    }

    // 更新最小频率
    updateMinFreq();
}

template<typename Key, typename Value>
void KLfuCache<Key, Value>::updateMinFreq() 
{
    minFreq_ = INT8_MAX;
    for (const auto& pair : freqToFreqList_) 
    {
        if (pair.second && !pair.second->isEmpty()) 
        {
            minFreq_ = std::min(minFreq_, pair.first);
        }
    }
    if (minFreq_ == INT8_MAX) 
        minFreq_ = 1;
}
```

#### 设置最大平均访问次数的值解决了什么问题？

1. 防止某一个缓存的访问频次无限增加，而导致的计数溢出。
2. 旧的热点缓存，也就是该数据之前的访问频次很高，但是现在不再被访问了，也能够保证他在每次访问缓存平均访问次数大于最大平均访问次数的时候减去一个固定的值，使这个过去的热点缓存的访问频次逐步降到最低，然后从内存中淘汰出去
3. 一定程度上是对新加入进来的缓存，也就是访问频次为1的数据缓存进行了保护，因为长时间没被访问的旧的数据不再会长期占据缓存空间，访问频率会逐步被降为小于1最终淘汰。

#### HashLfuCache

实现类似于HashLruCache，理论部分直接看上文，代码实现如下：

```
// 并没有牺牲空间换时间，他是把原有缓存大小进行了分片。
template<typename Key, typename Value>
class KHashLfuCache
{
public:
    KHashLfuCache(size_t capacity, int sliceNum, int maxAverageNum = 10)
        : sliceNum_(sliceNum > 0 ? sliceNum : std::thread::hardware_concurrency())
        , capacity_(capacity)
    {
        size_t sliceSize = std::ceil(capacity_ / static_cast<double>(sliceNum_)); // 每个lfu分片的容量
        for (int i = 0; i < sliceNum_; ++i)
        {
            lfuSliceCaches_.emplace_back(new KLfuCache<Key, Value>(sliceSize, maxAverageNum));
        }
    }

    void put(Key key, Value value)
    {
        // 根据key找出对应的lfu分片
        size_t sliceIndex = Hash(key) % sliceNum_;
        lfuSliceCaches_[sliceIndex]->put(key, value);
    }

    bool get(Key key, Value& value)
    {
        // 根据key找出对应的lfu分片
        size_t sliceIndex = Hash(key) % sliceNum_;
        return lfuSliceCaches_[sliceIndex]->get(key, value);
    }

    Value get(Key key)
    {
        Value value;
        get(key, value);
        return value;
    }

    // 清除缓存
    void purge()
    {
        for (auto& lfuSliceCache : lfuSliceCaches_)
        {
            lfuSliceCache->purge();
        }
    }

private:
    // 将key计算成对应哈希值
    size_t Hash(Key key)
    {
        std::hash<Key> hashFunc;
        return hashFunc(key);
    }

private:
    size_t capacity_; // 缓存总容量
    int sliceNum_; // 缓存分片数量
    std::vector<std::unique_ptr<KLfuCache<Key, Value>>> lfuSliceCaches_; // 缓存lfu分片容器
};
```

### ARC

核心思想：当访问的数据趋向于访问最近的内容，会更多地命中LRU list，这样会增大LRU的空间；当系统趋向于访问频繁访问的内容时，会更多地命中LFU list，这样会增加LFU的空间。

应用场景：主要应用于不同的访问模式(比如近期频繁访问和周期性访问交叉的场景)之间表现较好，因为他可以动态调整缓存分区的大小。

参考文章：

[https://blog.csdn.net/liuyun2113/article/details/12705057](https://blog.csdn.net/liuyun2113/article/details/12705057)

[https://blog.csdn.net/sbsbsb666666/article/details/130198264](https://blog.csdn.net/sbsbsb666666/article/details/130198264)

下面引入一些其他地方对其的解释：ps: MFU代表LFU，MRU代表LRU

![[缓存系统3.png]]

![[缓存系统4.png]]

相比于LFU和LRU其的优势在于：
![[缓存系统5.png]]

#### 执行过程剖析：

1. 整个Cache分为两部分，LRU和LFU各占一半，根据不同的访问模式动态适应调整`partition`分割指针的位置（也就是动态调整lfu和lru的大小），除此之外，LRU和LFU各自有一个`ghost list`（淘汰链表，分别用于存储从LRU和LFU中淘汰的数据）。总体结构如图：

![[代码随想录ARC结构图.jpg]]

更加详细结构流程图如下：（ps：`ARCCache`对象的`get`方法和`put`方法实际上调用的是LRU和LFU的`get`与`put`方法。 `frequency`指的是该数据的访问频率。）
![[代码随想录ARC流程图.png]]


2. 在缓存中查找客户端需要访问的数据，如果没有命中，表示缓存穿透，将需要访问的数据从从磁盘中取出，插入缓存中的`LRU`部分对应链表的尾部（ps：头插和尾插没有区别，看个人习惯）。
3. 如果命中（LRU部分存在该数据）且LFU链表中没有则判断该数据的访问次数是否大于`transformTime_`（自定义的变量，我这里设置为3，代表访问次数超过3次），若大于则将该数据插入`LFU`对应链表中（一个块至少需要读取`transformTime_`次，并且要是最近请求的数据，才会被存储到`LFU`中）。于是，该数据块不仅仅只保存在LRU的缓存目录中，也将保存到LFU中。如果命中且LFU链表中存在，则将数据重新放入LFU链表中对应位置（访问频次计数+1），这样，那些真正被频繁访问的页面将一直呆在缓存中，不会被冷数据的加入而误淘汰，不经常访问的数据会向链表头部移动，最终被淘汰出去。
4. 如果此时`LRU`缓存满了，则从`LRU`链表中淘汰表头部的数据，将淘汰数据的`key`放入`LRU`对应的`ghost list`。然后在`LRU`的链表尾部添加新数据。如果`ghost list`的元素满了，按照先进先出的方式淘汰`ghost list`中的元素头部元素，然后再从尾部插入元素。
5. 如未命中缓存的数据根据key发现在LRU对应的`ghost list`中，则表示幽灵命中，缓存系统就可以知道，这是一个刚刚淘汰的页面，而不是第一次读取或者说很久之前读取的一个页面。于是根据这个信息来调整内部的`partition`分割指针以适应当下的访问模式。上述迹象说明当前的`LRU`缓存太小了，于是将`partition`分割指针右移一位(也就是`LRU`缓存空间+1，`LFU`缓存空间-1），并将命中的`key`数据从`ghost`的中移除，将新数据从LRU链表尾部插入。
6. 反之如果未命中缓存的数据根据key发现在LFU对应的ghost中，则说明当前访问模式偏向于频繁访问经常被访问的那一些数据，说明当下LFU缓存空间太小了，对应`partition`分割指针左移一位，并将命中的`key`数据从`ghost`的中移除，将新数据从LRU链表尾部插入。

#### ARC代码实现

该代码量较大，就不在文章中展示了，请移步：[https://github.com/youngyangyang04/KamaCache/blob/main/KArcCache.h](https://github.com/youngyangyang04/KamaCache/blob/main/KArcCache.h)

# 缓存策略对比
### 缓存策略对比

不管是使用LRU、LFU和ARC算法都是为了优化缓存命中率，确保最热数据优先留存在内存中，而将不常用或较久未使用的数据被淘汰。三种缓存策略各有优缺点，对比如下：

|                                 |                                                                                  |                                                                   |                                                                                               |
| ------------------------------- | -------------------------------------------------------------------------------- | ----------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
|                                 | 优势                                                                               | 缺陷                                                                | 应用场景                                                                                          |
| LRU（Least Recently Used）        | - 实现简单，基于时间的自然访问淘汰策略<br>- 适合短期热点数据，命中率高<br>- 易于于其他缓存机制结构结合（如双链表实现）               | - 循环缓存问题：对于扫描型访问，可能频繁淘汰热点数据，导致缓存抖动<br>- 不能很好处理长期热点数据和周期性热点数据      | 适合访问模式稳定、短期热点突出的场景，如:<br><br>- web缓存（浏览器页面）<br>- 文件系统缓存（例如内存有限的小型文件系统）<br>- 数据库的缓冲区管理（访问历史稳定） |
| LFU（Least Frequenty Used）       | - 更关注访问频率，能够保留长期使用的重要数据。<br>- 对稳定热点数据的命中率更高                                      | - 冷启动问题：新数据可能在尚未证明重要性前被淘汰<br>- 数据频率统计更新有额外的开销<br>- 不适合访问模式变化频繁的场景 | 适合访问频率稳定、长期热点数据占主导的场景，如：<br><br>- 热门资源（视频、音乐）的推荐系统缓存<br>- 机器学习中训练数据的缓存                        |
| ARC（Adaptive Replacement Cache） | - 结合LRU和LFU的优点，动态调整策略以适应短期和长期热点。<br>- 抗抖动能力强，能快速适应缓存 模式的变化。<br>- 高命中率，适合多样化的访问模式 | - 实现复杂，涉及较多数据结构，开销大<br>- 动态调整LFU和LRU缓存比例增加了计算成本。                  | 适合访问模式多变的场景：<br><br>- 数据库缓冲区管理<br>- 文件系统缓存（如ZFS）<br>- 混合型访问模式（扫描+热点）的应用                       |
# 缓存命中率测试
## 缓存测试结果展示

该测试代码只是尽可能地模拟真实的访问场景，但是跟真实的场景仍存在一定差距，测试结果仅供参考。
![[缓存系统6.png]]  

## 缓存策略测试分析

### 1. 测试场景概述

#### 1.1 热点数据访问测试 (testHotDataAccess)

##### 1. 测试代码：

```
void testHotDataAccess() {
    std::cout << "\n=== 测试场景1：热点数据访问测试 ===" << std::endl;
    
    const int CAPACITY = 20;         // 缓存容量
    const int OPERATIONS = 500000;   // 总操作次数
    const int HOT_KEYS = 20;         // 热点数据数量
    const int COLD_KEYS = 5000;      // 冷数据数量
    
    KamaCache::KLruCache<int, std::string> lru(CAPACITY);
    KamaCache::KLfuCache<int, std::string> lfu(CAPACITY);
    KamaCache::KArcCache<int, std::string> arc(CAPACITY);

    // 生成随机数种子，类似与rand()，但是更优
    std::random_device rd;
    std::mt19937 gen(rd());
    
    // 基类指针指向派生类对象（多态）
    std::array<KamaCache::KICachePolicy<int, std::string>*, 3> caches = {&lru, &lfu, &arc};
    std::vector<int> hits(3, 0); // 记录三种策略分别缓存命中次数
    std::vector<int> get_operations(3, 0); // 三种策略测试分别get访问缓存总次数
    std::vector<std::string> names = {"LRU", "LFU", "ARC"};

    // 为所有的缓存对象进行相同的操作序列测试
    for (int i = 0; i < caches.size(); ++i) {
        // 先预热缓存，插入一些数据
        for (int key = 0; key < HOT_KEYS; ++key) {
            std::string value = "value" + std::to_string(key);
            caches[i]->put(key, value);
        }
        
        // 交替进行put和get操作，模拟真实场景
        for (int op = 0; op < OPERATIONS; ++op) {
            // 大多数缓存系统中读操作比写操作频繁
            // 所以设置30%概率进行写操作
            bool isPut = (gen() % 100 < 30); 
            int key;
            
            // 70%概率访问热点数据，30%概率访问冷数据
            if (gen() % 100 < 70) {
                key = gen() % HOT_KEYS; // 热点数据
            } else {
                key = HOT_KEYS + (gen() % COLD_KEYS); // 冷数据
            }
            
            if (isPut) {
                // 执行put操作
                std::string value = "value" + std::to_string(key) + "_v" + std::to_string(op % 100);
                caches[i]->put(key, value);
            } else {
                // 执行get操作并记录命中情况
                std::string result;
                get_operations[i]++;
                if (caches[i]->get(key, result)) {
                    hits[i]++;
                }
            }
        }
    }

    // 打印测试结果
    printResults("热点数据访问测试", CAPACITY, get_operations, hits);
}
```

##### 2. 测试代码解释

**测试特征：**

- 缓存容量：20
- 操作次数：500,000
- get访问缓存 和 put添加缓存比例，为了模拟真实场景，测试做了以下安排：

- 先进行预热，加载热点数据
- 测试过程中get和put随机交叉进行，put发生的概率是30%，get发生的概率是70%（反应了大多数缓存系统中读操作通常比写操作更频繁的特性）。
- 实时记录了命中次数和总读取次数。

- 数据访问偏向性模拟（模拟现实应用中的“二八法则”- 少量数据被大量访问，大量数据被少量访问）：

- 70% 访问热点数据（key范围：0-19）
- 30% 访问冷数据（key范围：20-4999）

**测试目的：**

- 验证缓存对频繁访问的少量数据的处理能力
- 测试在大量冷数据干扰下的缓存性能

**有利策略：**

- LFU：因为能准确跟踪访问频率，对热点数据有很好的保留能力
- ARC：能够通过自适应机制在列表中保留频繁访问的数据

  

#### 1.2 循环扫描测试 (testLoopPattern)

##### 1. 测试代码：

```
void testLoopPattern() {
    std::cout << "\n=== 测试场景2：循环扫描测试 ===" << std::endl;
    
    const int CAPACITY = 50;          // 缓存容量
    const int LOOP_SIZE = 500;        // 循环范围大小
    const int OPERATIONS = 200000;    // 总操作次数
    
    KamaCache::KLruCache<int, std::string> lru(CAPACITY);
    KamaCache::KLfuCache<int, std::string> lfu(CAPACITY);
    KamaCache::KArcCache<int, std::string> arc(CAPACITY);

    std::array<KamaCache::KICachePolicy<int, std::string>*, 3> caches = {&lru, &lfu, &arc};
    std::vector<int> hits(3, 0);
    std::vector<int> get_operations(3, 0);
    std::vector<std::string> names = {"LRU", "LFU", "ARC"};

    std::random_device rd;
    std::mt19937 gen(rd());

    // 为每种缓存算法运行相同的测试
    for (int i = 0; i < caches.size(); ++i) {
        
        // 先预热一部分数据（只加载20%的数据）
        for (int key = 0; key < LOOP_SIZE / 5; ++key) {
            std::string value = "loop" + std::to_string(key);
            caches[i]->put(key, value);
        }
        
        // 设置循环扫描的当前位置
        int current_pos = 0;
        
        // 交替进行读写操作，模拟真实场景
        for (int op = 0; op < OPERATIONS; ++op) {
            // 20%概率是写操作，80%概率是读操作
            bool isPut = (gen() % 100 < 20);
            int key;
            
            // 按照不同模式选择键
            if (op % 100 < 60) {  // 60%顺序扫描
                key = current_pos;
                current_pos = (current_pos + 1) % LOOP_SIZE;
            } else if (op % 100 < 90) {  // 30%随机跳跃
                key = gen() % LOOP_SIZE;
            } else {  // 10%访问范围外数据
                key = LOOP_SIZE + (gen() % LOOP_SIZE);
            }
            
            if (isPut) {
                // 执行put操作，更新数据
                std::string value = "loop" + std::to_string(key) + "_v" + std::to_string(op % 100);
                caches[i]->put(key, value);
            } else {
                // 执行get操作并记录命中情况
                std::string result;
                get_operations[i]++;
                if (caches[i]->get(key, result)) {
                    hits[i]++;
                }
            }
        }
    }

    printResults("循环扫描测试", CAPACITY, get_operations, hits);
}
```

##### 2. 测试代码解释：

**测试特征：**

- 缓存容量：30
- 循环范围：500
- 操作次数：80000
- get访问缓存 和 put添加缓存比例，为了模拟真实场景，测试做了以下安排：

- 只预热了20%的数据，而不是全部500个键值对。这更符合真实场景，缓存通常是逐渐填充的，而不是一次性完全填满。
- 测试过程中get和put随机交叉进行，put发生的概率是20%，get发生的概率是80%（循环扫描场景读中，读操作更频繁）。
- 实时记录了命中次数和总读取次数。

- 访问模式：

- 60% 顺序扫描（每次 key + 1 % 循环范围）
- 30% 随机跳跃（循环范围内的随机数）
- 10% 范围外访问（循环范围外的随机数）

**测试目的：**

- 验证缓存在顺序访问模式下的表现
- 测试缓存对周期性访问模式的适应能力
- 评估缓存在数据局部性变化时的性能

**策略对比：**

- LRU的弱点：在循环扫描模式下，LRU容易发生"缓存抖动"，因为最近使用的数据可能立即被新的扫描数据替换。
- LFU的表现：循环扫描模式下，如果某些数据被重复扫描多次，LFU可能会表现更好。
- ARC的优势：ARC通过同时考虑访问频率和时间，可能在这种混合访问模式下有更平衡的表现。

#### 1.3 工作负载剧烈变化测试 (testWorkloadShift)

##### 1. 测试代码：

```
void testWorkloadShift() {
    std::cout << "\n=== 测试场景3：工作负载剧烈变化测试 ===" << std::endl;
    
    const int CAPACITY = 30;            // 缓存容量
    const int OPERATIONS = 80000;       // 总操作次数
    const int PHASE_LENGTH = OPERATIONS / 5;  // 每个阶段的长度
    
    KamaCache::KLruCache<int, std::string> lru(CAPACITY);
    KamaCache::KLfuCache<int, std::string> lfu(CAPACITY);
    KamaCache::KArcCache<int, std::string> arc(CAPACITY);

    std::random_device rd;
    std::mt19937 gen(rd());
    std::array<KamaCache::KICachePolicy<int, std::string>*, 3> caches = {&lru, &lfu, &arc};
    std::vector<int> hits(3, 0);
    std::vector<int> get_operations(3, 0);
    std::vector<std::string> names = {"LRU", "LFU", "ARC"};

    // 为每种缓存算法运行相同的测试
    for (int i = 0; i < caches.size(); ++i) {
        // 先预热缓存，只插入少量初始数据
        for (int key = 0; key < 30; ++key) {
            std::string value = "init" + std::to_string(key);
            caches[i]->put(key, value);
        }
        
        // 进行多阶段测试，每个阶段有不同的访问模式
        for (int op = 0; op < OPERATIONS; ++op) {
            // 确定当前阶段
            int phase = op / PHASE_LENGTH;
            
            // 每个阶段的读写比例不同 - 优化后的比例
            int putProbability;
            switch (phase) {
                case 0: putProbability = 15; break;  // 阶段1: 热点访问，15%写入更合理
                case 1: putProbability = 30; break;  // 阶段2: 大范围随机，降低写比例为30%
                case 2: putProbability = 10; break;  // 阶段3: 顺序扫描，10%写入保持不变
                case 3: putProbability = 25; break;  // 阶段4: 局部性随机，微调为25%
                case 4: putProbability = 20; break;  // 阶段5: 混合访问，调整为20%
                default: putProbability = 20;
            }
            
            // 确定是读还是写操作
            bool isPut = (gen() % 100 < putProbability);
            
            // 根据不同阶段选择不同的访问模式生成key - 优化后的访问范围
            int key;
            if (op < PHASE_LENGTH) {  // 阶段1: 热点访问 - 减少热点数量从10到5，使热点更集中
                key = gen() % 5;
            } else if (op < PHASE_LENGTH * 2) {  // 阶段2: 大范围随机 - 范围从1000减小到400，更适合20大小的缓存
                key = gen() % 400;
            } else if (op < PHASE_LENGTH * 3) {  // 阶段3: 顺序扫描 - 保持100个键
                key = (op - PHASE_LENGTH * 2) % 100;
            } else if (op < PHASE_LENGTH * 4) {  // 阶段4: 局部性随机 - 优化局部性区域大小
                // 产生5个局部区域，每个区域大小为15个键，与缓存大小20接近但略小
                int locality = (op / 800) % 5;  // 调整为5个局部区域
                key = locality * 15 + (gen() % 15);  // 每区域15个键
            } else {  // 阶段5: 混合访问 - 增加热点访问比例
                int r = gen() % 100;
                if (r < 40) {  // 40%概率访问热点（从30%增加）
                    key = gen() % 5;  // 5个热点键
                } else if (r < 70) {  // 30%概率访问中等范围
                    key = 5 + (gen() % 45);  // 缩小中等范围为50个键
                } else {  // 30%概率访问大范围（从40%减少）
                    key = 50 + (gen() % 350);  // 大范围也相应缩小
                }
            }
            
            if (isPut) {
                // 执行写操作
                std::string value = "value" + std::to_string(key) + "_p" + std::to_string(phase);
                caches[i]->put(key, value);
            } else {
                // 执行读操作并记录命中情况
                std::string result;
                get_operations[i]++;
                if (caches[i]->get(key, result)) {
                    hits[i]++;
                }
            }
        }
    }

    printResults("工作负载剧烈变化测试", CAPACITY, get_operations, hits);
}
```

##### 2. **测试代码解释：**

**测试特征：**

- 缓存容量：30
- 操作次数：80,000
- get访问缓存 和 put添加缓存比例，为了模拟真实场景，测试做了以下安排：

- 读写交替进行，每个阶段的读写比例不同。
- 阶段1: 热点访问，15%写入。
- 阶段2: 大范围随机，写比例30%。
- 阶段3: 顺序扫描，10%写入。
- 阶段4: 局部性随机，微调为25%。
- 阶段5: 混合访问，调整为20%。

- 五个不同访问阶段（每个阶段操作80,000 / 5 次）：

1. 热点访问（少量key密集访问）
2. 大范围随机访问（400个key随机访问）
3. 顺序扫描（100个key，按顺序递增访问）
4. 局部性随机访问（分区域随机）
5. 混合访问模式（40%概率访问热点数据，30%概率随机访问中等范围数据，30%概率访问大范围数据）

**测试目的：**

- 评估缓存在工作负载突变时的适应能力
- 测试不同缓存策略在复杂访问模式下的表现
- 验证缓存算法的稳定性和自适应能力

**策略对比：**

- LRU可能在工作负载变化时迅速调整，但缺乏长期记忆
- LFU可能保持长期记忆，但调整较慢
- ARC应该能在这两者间取得平衡

### 2. 测试设计特点

##### 2.1 数据操作分离

- 将put和get操作分开执行
- 避免了put后立即get导致的虚假高命中率

##### 2.2 动态数据范围

- 通过动态改变数据访问范围
- 模拟真实环境中数据分布的变化

##### 2.3 多样化访问模式

- 结合顺序访问、随机访问和局部性访问
- 更接近实际应用场景

### 3. 预期效果分析

##### 3.1 LRU 策略

- 优势场景：

- 具有时间局部性的访问模式
- 工作集大小小于缓存容量的情况

- 劣势场景：

- 循环扫描大量数据
- 频繁访问的数据集大于缓存容量

##### 3.2 LFU 策略

- 优势场景：

- 稳定的热点数据访问
- 访问频率分布稳定的工作负载

- 劣势场景：

- 访问模式突变
- 临时突发的高频访问

##### 3.3 ARC 策略

- 优势场景：

- 复杂的混合访问模式
- 工作负载频繁变化
- 同时存在时间和频率局部性

- 劣势场景：

- 极端单一的访问模式（纯随机或纯顺序）

### 4. 测试结果分析要点

- 关注命中率的绝对值
- 观察不同策略在场景切换时的适应速度
- 比较策略间的性能差异
- 验证各策略在其优势场景中的表现

# 简历写法
## 简历写法

### 一般简历写法

#### cpp-高并发缓存

**项目描述：**基于C++实现的一个线程安全的高并发缓存系统，支持多种缓存替换策略（LRU、LFU、ARC）。项目开发注重系统的并发性能优化和缓存策略改进，以提高在高并发场景下的响应速度与命中率。

**主要工作**

- 实现多种缓存替换策略，适配不同的访问模式和业务场景；
- 实现LRU和LFU的缓存分片，降低锁争用，提升高并发访问下的性能；
- 实现LRU-k优化，防止热点数据被冷数据替换，减少缓存污染问题；
- 实现LFU引入最大平均访问频次，淘汰旧的热点数据，提升缓存的整体利用率；
- 实现ARC策略，动态调整LRU和LFU的权重比例，提升复杂环境下的缓存命中率；
- 通过互斥锁和原子操作实现多线程下的线程安全；

### 进阶简历写法

**高并发缓存系统**

**项目描述：** 基于C++实现的**线程安全**的**高并发缓存系统**，支持多种缓存替换策略（**LRU**、**LFU**、**ARC**）。项目开发注重系统的**并发性能优化**和缓存策略改进，以提高在**高并发场景**下的响应速度与命中率。

**个人工作：**

- 实现了**多种缓存替换策略**，适配不同的访问模式和业务场景；
- 实现了**LRU**和**LFU**的缓存分片，降低锁争用，提升**高并发访问**下的性能；
- 实现了**LRU-k优化**，防止热点数据被冷数据替换，减少缓存污染问题；
- 实现了**LFU**引入最大平均访问频次，淘汰旧的热点数据，提升缓存的整体利用率；
- 实现了**ARC策略**，动态调整**LRU**和**LFU**的权重比例，提升复杂环境下的缓存命中率；
- 通过**互斥锁**和**原子操作**实现多线程下的线程安全；

**项目难点：**

1. **线程安全**的实现，确保在**高并发**环境下数据的一致性和完整性；
2. **缓存分片**策略的设计，以减少锁争用并提高并发性能；
3. **LRU-k优化**的实现，以解决热点数据被冷数据替换的问题；
4. **LFU**策略中最大平均访问频次的计算和应用，以提升缓存利用率；
5. **ARC策略**的动态权重调整算法，以适应不同业务场景下的缓存需求；

**个人收获：**

1. 深入理解了**多线程编程**和**线程同步**机制，包括互斥锁和原子操作；
2. 掌握了**缓存替换策略**的设计与实现，包括**LRU**、**LFU**和**ARC**；
3. 提升了对**高并发系统设计**的理解，特别是在缓存系统中的应用；
4. 学习了如何通过**性能分析**和**优化**来提升系统在高并发场景下的表现；
5. 增强了对**C++语言在系统级编程**中的运用能力，特别是在内存管理和并发控制方面。

# 常见面试题
这里的答案只是抛转引玉，答案大家可以根据自己的理解整理。
部分题目参考：[https://blog.csdn.net/qq_40374604/article/details/128123061](https://blog.csdn.net/qq_40374604/article/details/128123061)
#### 1. 你的缓存是什么？你的缓存机制是如何设计的？它用来缓存什么东西？具体在程序中用于哪些场景？

答：缓存是将高频访问的数据暂存到内存中，是加速数据访问的存储，降低延迟，提高吞吐量。

采用LRU，LFU等策略去实现缓存的。

用于缓存一些频繁访问的数据，比如网页等文件资源、数据库的热点数据等。

比如客户端向服务端请求某项文件资源时，服务端程序需要先将该文件资源从磁盘中读出到内存缓冲区中，然后再将其从内存缓冲区中发送给客户端。

如果有了缓存机制，对于已经缓存了的文件，在客户端请求后，服务端就不用再向磁盘读取了，可以从缓存中直接取出文件传输给客户端。避免了读取磁盘带来的开销。高并发场景下频繁读取磁盘也会造成较大的性能开销。

#### 2. 你用缓存机制缓存了数据库数据吗？

- 缓存是可以用于缓存数据库的数据的，因为数据库访问（特别是复杂查询）通常非常慢
- 数据变更频率较低，但访问频率高
- 如果数据是“热点数据”，就更适合缓存

#### 3. 什么是缓存？

答：缓存，就是数据交换的缓冲区，是一种用于临时存储数据的高效存储机制，其主要目的是加快访问速度、减轻后台系统压力，从而提升整体性能。我们平时说的缓存大多是指内存。目的是，把读写速度慢的介质的数据保存在读写速度快的介质中（这里的快与慢是相对概念），从而提高读写速度，减少时间消耗。例如：

- CPU高速缓存：告诉缓存的读写速度远高于内存。

- CPU读数据时，如果在高速缓存中找到所需数据，就不需要读内存
- CPU写数据时，先写到高速缓存，再写回内存。

- 磁盘缓存：磁盘缓存其实就把常用的磁盘数据保存在内存中，内存读写速度也是远高于磁盘的。

- 读数据时从内存中读取
- 写数据时，可先写回内存，定时或定量写回到磁盘，或者是同步写回。

  

#### 4. 请说说有哪些缓存算法？是否能手写一下LRU代码的实现？

缓存算法中，比较常见的如下：

- FIFO(先进先出)
- LRU(最近最少使用)
- LFU(最不经常使用)
- ARC(自适应替换)

LRU代码实现如下：

```
#include <unordered_map>
#include <list>
#include <iostream>

class LRUCache {
private:
    struct CacheNode {
        int key;
        int value;
        CacheNode(int k, int v) : key(k), value(v) {}
    };

    int capacity;
    std::list<CacheNode> cacheList; // 双向链表，存储缓存数据
    std::unordered_map<int, std::list<CacheNode>::iterator> cacheMap; // 哈希表，存储键和对应在双向链表中的迭代器

public:
    LRUCache(int capacity) : capacity(capacity) {}

    int get(int key) {
        auto it = cacheMap.find(key);
        if (it == cacheMap.end()) {
            return -1; // 未找到
        }
        // 将访问的节点移动到双向链表的头部
        cacheList.splice(cacheList.begin(), cacheList, it->second);
        return it->second->value;
    }

    void put(int key, int value) {
        auto it = cacheMap.find(key);
        if (it != cacheMap.end()) {
            // 如果键已存在，更新值，并将其移动到双向链表的头部
            it->second->value = value;
            cacheList.splice(cacheList.begin(), cacheList, it->second);
            return;
        }

        if (cacheMap.size() == capacity) {
            // 如果缓存已满，移除双向链表的尾节点
            auto last = cacheList.back();
            cacheMap.erase(last.key);
            cacheList.pop_back();
        }

        // 添加新节点到双向链表的头部
        cacheList.emplace_front(key, value);
        cacheMap[key] = cacheList.begin();
    }
};

int main() {
    LRUCache cache(2);
    cache.put(1, 1);
    cache.put(2, 2);
    std::cout << cache.get(1) << std::endl; // 返回 1
    cache.put(3, 3);                        // 淘汰键2
    std::cout << cache.get(2) << std::endl; // 返回 -1 (未找到)
    cache.put(4, 4);                        // 淘汰键1
    std::cout << cache.get(1) << std::endl; // 返回 -1 (未找到)
    std::cout << cache.get(3) << std::endl; // 返回 3
    std::cout << cache.get(4) << std::endl; // 返回 4
    return 0;
}
```

  

#### 5. 常见的缓存工具和框架有哪些？

在C++开发中，常见的缓存工具和框架如下：

- Boost.Cache：提供灵活的缓存管理，支持LRU和LFU等策略。
- Redis：支持多种数据结构的分布式缓存。也是最主流最常用的。
- Intel TBB Cache：提供线程安全的缓存实现，支持并发访问。

  

#### 6. 为什么需要实现这个项目？使用缓存的目标是什么？

**思路**：阐述高并发场景下缓存的意义，例如提升性能、降低数据库压力、优化用户体验等。

#### 7. 多线程下如何保证线程安全？你使用了什么技术或方法来实现线程安全？

**思路**：

- 使用 `std::mutex`、`std::shared_mutex` 等同步机制避免数据竞争。
- 如果有分片优化，可以提到分片缓存降低锁粒度，减少锁竞争。
- 使用原子操作（如 `std::atomic`）优化性能。

#### 8. 实现了哪些缓存策略？如何选择？各自的优缺点是什么？

**思路**：

- 说明实现了 LRU、LFU、ARC 策略及其优化：

- **LRU 分片**：减少并发访问的锁冲突。
- **LFU 分片**：降低访问频率更新的竞争。
- **ARC**：动态平衡命中率和访问频率的需求。

- 对比这些策略的应用场景和缺点。

  

#### 9. 如何存储缓存数据？数据结构是如何设计的？

**思路**：

- 提到使用 `std::unordered_map`（哈希表）作为快速查找的基础。
- 使用双向链表（如 `std::list`）维护访问顺序，实现 LRU 或 LFU 的高效淘汰。

#### 10. 每种缓存策略的时间复杂度和空间复杂度分别是多少？

**思路**：

- LRU：O(1) 查找、O(1) 插入/删除。
- LFU：O(log 1) 更新频率。
- ARC：动态调整需要额外的复杂性分析。

#### 11. 你对 LRU 和 LFU 策略做了哪些优化？如何验证这些优化的效果？

**思路**：

- LRU 分片：减少锁冲突。
- LRU-k：解决热点数据被冷数据挤出问题。
- LFU 增加最大平均访问频率：避免历史热点长期占用缓存。

#### 12. 多线程高并发下，如何优化缓存的性能？

**思路**：

- 缓存分片：每个分片使用单独的锁，降低锁粒度。
- 读写分离：大量读操作可使用 `std::shared_mutex` 提升性能。
- 非阻塞操作：可以提到 CAS（Compare-And-Swap）优化。

#### 13. 如何评估缓存的性能？测试指标有哪些？

**思路**：

- 测试指标包括命中率、吞吐量、延迟。
- 通过压测工具（如 Webbench、ab）模拟高并发场景，验证性能是否达标。

  

#### 14. 问题：如何选择锁机制？为什么要降低锁的粒度？

**思路：**

- 锁的粒度越小，锁冲突越低，但管理复杂度增加。
- 使用读写锁时，读多写少场景下可以显著提高性能。

#### 15. 如何同时支持 LRU、LFU 和 ARC 策略？用户如何选择不同的策略？

**思路**：

- 提供抽象基类或接口，设计多种策略的实现。
- 用户可以通过配置或运行时参数选择不同的策略。

  

#### 16. 如果需要新增一种缓存策略，如何设计？

**思路**：

- 基于面向对象编程的思想，新增策略只需继承接口并实现。

#### 17. 如何应对缓存穿透、缓存击穿和缓存雪崩问题？

**思路**：

- **缓存穿透**：使用布隆过滤器过滤无效查询。
- **缓存击穿**：对热点数据加锁，避免缓存失效后高并发访问后端存储。
- **缓存雪崩**：错开缓存过期时间，或设置合理的回源策略。

  

#### 18. Redis有哪些淘汰策略

Redis的淘汰策略有以下几种：

1. LRU算法：当内存不足以容纳新写入数据时，移除缓存中最近最少访问的数据
2. LFU算法：当内存不足以容纳新写入数据时，移除缓存中最不经常访问的数据
3. FIFO算法：最早放入缓存的数据最先被删除
4. Random算法：随机移除某个键

当涉及到设置了过期时间的数据时，还有以下策略：

1. volatile-lru：从设置了过期时间的数据中选择最近最少使用的数据淘汰。
2. volatile-lfu：从设置了过期时间的数据中选择最不经常访问的数据淘汰。
3. volatile-random：从设置了过期时间的数据中随机选择数据淘汰。
4. volatile-lfu：从设置了过期时间的数据中选择过期时间最近的键淘汰。

  

#### 19. 什么是缓存污染，你是如何减少这个问题的？

**思路：**

- **缓存污染定义**：指不重要的数据占据了缓存空间，从而挤出更重要的、经常访问的数据。
- **解决措施**：

- 使用 **LRU-k**：缓存只淘汰短期未访问的数据，防止热点数据被冷门数据挤出。
- 增加缓存淘汰策略的灵活性，例如结合访问频率和时间综合判断。

#### 20. 你是如何实现缓存分片的？

**思路：**

- 将缓存划分成多个片段，每个片段有独立的数据结构和锁。
- 根据键值的 **哈希值** 确定对应的分片，从而减少锁争用。
- 每个分片独立管理内存空间和替换策略，以便最大限度优化并发性能。

  

#### 21. 在C++中，原子操作是如何实现线程安全的？

**思路：**

- 使用 **std::atomic** 提供的操作，例如 `std::atomic<int>`，避免因竞态条件导致数据不一致。
- 底层实现：通过硬件提供的 **原子指令**（如 CPU 的 CAS）来确保操作的不可分割性。
- 优势：无需加锁，减少线程之间的阻塞，提高性能。

#### 22. 你是如何动态调整ARC策略中的权重比例的？

**思路：**

- 根据实时监控的命中率（命中频率 vs. 未命中频率）进行调整。
- 逻辑实现：

- **LRU 部分权重**：针对短期热点数据。
- **LFU 部分权重**：针对长期热点数据。
- 动态调整：命中率下降时，增大 LFU 权重（防止频繁访问的数据被替换）；反之增大 LRU 权重。

- 示例：实现一个自适应机制，通过调节阈值动态切换缓存分区的占比。

#### 23. 在设计高并发缓存系统时，你考虑了哪些性能指标？

**思路：**

- **响应速度**：缓存访问的延迟应尽可能低（如纳秒级）。
- **命中率**：缓存被访问时，命中次数占总访问次数的比例。
- **吞吐量**：系统每秒可以处理的请求数量。
- **资源利用率**：内存占用、CPU 消耗，以及网络 I/O 等的效率。
- **扩展性**：是否可以轻松扩展缓存容量或支持更高并发量。

#### 24. 你是如何优化缓存系统以提高高并发场景下的响应速度的？

**思路：**

- 使用 **缓存分片**，减少全局锁的使用。
- 优化替换策略，减少复杂度。
- 利用 **本地性原则**（如将常用缓存分配在线程本地存储中）。
- 调整数据结构，例如使用 **哈希链表** 或 **跳表** 以减少查询时间复杂度。

#### 25. 你是如何确保在高并发环境下数据的一致性的？

思路：

- 使用 **细粒度锁**：只锁定需要操作的部分数据，而不是全局锁定。
- 使用原子操作确保对共享数据的访问安全。

#### 26. 在C++中，你是如何管理内存的？

思路：

- 使用 **智能指针**：`std::shared_ptr` 和 `std::unique_ptr` 自动管理对象生命周期，避免内存泄漏。
- 使用 **RAII** 原则：在构造时分配资源，析构时自动释放资源。
- 定期清理：缓存中淘汰的对象及时清除，以释放内存。
- 预分配内存池：减少频繁的内存分配操作，优化性能。

  

#### 27. 你是如何实现缓存的预热功能的？

思路：

- **预加载策略**：在系统启动时，分析历史数据，加载常用或热点数据到缓存中。
- **实现方式**：提供接口将静态数据加载到缓存。例如：

```
复制代码
cache.put("key", "value"); // 手动加载
```

- 优势：减少启动后首次访问的延迟，提高用户体验。

  

#### 28. 在实现缓存分片时，你是如何确定分片数量的？

思路：

- 根据系统的 **CPU 核心数**，设置分片数量与核心数一致，利用多核性能。
- 根据预期的 **并发量** 和 **负载**，通过压力测试确定分片数量，以确保负载均衡。
- 分片数与哈希函数结合，避免数据分布不均。

  

#### 29. 你是如何确保缓存系统在面对突发流量时的稳定性的？

**思路：**

- **负载均衡**：通过多个缓存节点分担请求压力。
- **熔断机制**：当缓存压力过大时，临时降级，直接返回默认值或空值。
- **提前扩容：**在高并发场景下，动态扩展缓存容量或缓存节点。